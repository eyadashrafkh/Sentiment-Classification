{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config \"default\", \"dictionary\", \"ptb\"\n",
    "default_dataset = load_dataset(\"sst\", \"default\")\n",
    "# dictionary_dataset = load_dataset(\"sst\", \"dictionary\")\n",
    "# ptb_dataset = load_dataset(\"sst\", \"ptb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Default:\",default_dataset)\n",
    "# print(\"Dictionary:\",dictionary_dataset)\n",
    "# print(\"PTB:\",ptb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence    label  \\\n",
      "0  The Rock is destined to be the 21st Century 's...  0.69444   \n",
      "1  The gorgeously elaborate continuation of `` Th...  0.83333   \n",
      "2  Singer\\/composer Bryan Adams contributes a sle...  0.62500   \n",
      "3  You 'd think by now America would have had eno...  0.50000   \n",
      "4               Yet the act is still charming here .  0.72222   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  The|Rock|is|destined|to|be|the|21st|Century|'s...   \n",
      "1  The|gorgeously|elaborate|continuation|of|``|Th...   \n",
      "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...   \n",
      "3  You|'d|think|by|now|America|would|have|had|eno...   \n",
      "4               Yet|the|act|is|still|charming|here|.   \n",
      "\n",
      "                                                tree  \n",
      "0  70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|5...  \n",
      "1  71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|5...  \n",
      "2  72|71|71|70|68|68|67|67|66|63|62|62|60|60|58|5...  \n",
      "3  36|35|34|33|33|32|30|29|27|26|25|24|23|23|22|2...  \n",
      "4          15|13|13|10|9|9|11|12|10|11|12|14|14|15|0  \n",
      "                                            sentence    label  \\\n",
      "0                     Effective but too-tepid biopic  0.51389   \n",
      "1  If you sometimes like to go to the movies to h...  0.73611   \n",
      "2  Emerges as something rare , an issue movie tha...  0.86111   \n",
      "3  The film provides some great insight into the ...  0.59722   \n",
      "4  Offers that rare combination of entertainment ...  0.83333   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                     Effective|but|too-tepid|biopic   \n",
      "1  If|you|sometimes|like|to|go|to|the|movies|to|h...   \n",
      "2  Emerges|as|something|rare|,|an|issue|movie|tha...   \n",
      "3  The|film|provides|some|great|insight|into|the|...   \n",
      "4  Offers|that|rare|combination|of|entertainment|...   \n",
      "\n",
      "                                                tree  \n",
      "0                                      6|6|5|5|7|7|0  \n",
      "1  40|39|38|37|36|34|33|32|32|31|30|30|29|28|26|2...  \n",
      "2  42|41|40|40|43|38|37|37|36|34|31|31|32|30|30|2...  \n",
      "3  50|50|47|46|45|45|44|42|41|41|40|36|36|37|38|3...  \n",
      "4  16|14|13|13|12|10|10|11|17|11|12|15|14|15|16|17|0  \n",
      "                                            sentence    label  \\\n",
      "0  It 's a lovely film with lovely performances b...  0.79167   \n",
      "1  No one goes unindicted here , which is probabl...  0.51389   \n",
      "2  And if you 're not nearly moved to tears by a ...  0.76389   \n",
      "3                   A warm , funny , engaging film .  0.88889   \n",
      "4  Uses sharp humor and insight into human nature...  0.80556   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  It|'s|a|lovely|film|with|lovely|performances|b...   \n",
      "1  No|one|goes|unindicted|here|,|which|is|probabl...   \n",
      "2  And|if|you|'re|not|nearly|moved|to|tears|by|a|...   \n",
      "3                   A|warm|,|funny|,|engaging|film|.   \n",
      "4  Uses|sharp|humor|and|insight|into|human|nature...   \n",
      "\n",
      "                                                tree  \n",
      "0  25|23|21|20|20|19|17|17|16|14|14|15|24|15|16|1...  \n",
      "1  24|24|22|19|19|20|18|16|16|15|14|14|23|15|17|1...  \n",
      "2  47|45|44|41|41|42|39|38|38|37|35|35|34|34|33|3...  \n",
      "3          14|12|12|13|11|9|9|10|10|11|15|13|14|15|0  \n",
      "4  46|43|41|41|42|40|39|39|38|37|36|36|35|33|31|3...  \n"
     ]
    }
   ],
   "source": [
    "train_def = default_dataset[\"train\"].to_pandas()\n",
    "test_def = default_dataset[\"test\"].to_pandas()\n",
    "val_def = default_dataset[\"validation\"].to_pandas()\n",
    "print(train_def.head())\n",
    "print(test_def.head())\n",
    "print(val_def.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = dictionary_dataset[\"dictionary\"].to_pandas()\n",
    "# print(dic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ptb = ptb_dataset[\"train\"].to_pandas()\n",
    "# test_ptb = ptb_dataset[\"test\"].to_pandas()\n",
    "# val_ptb = ptb_dataset[\"validation\"].to_pandas()\n",
    "# print(train_ptb.head())\n",
    "# print(test_ptb.head())\n",
    "# print(val_ptb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use the default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    The Rock is destined to be the 21st Century 's...\n",
      "1    The gorgeously elaborate continuation of `` Th...\n",
      "2    Singer\\/composer Bryan Adams contributes a sle...\n",
      "3    You 'd think by now America would have had eno...\n",
      "4                 Yet the act is still charming here .\n",
      "Name: sentence, dtype: object\n",
      "0    0.69444\n",
      "1    0.83333\n",
      "2    0.62500\n",
      "3    0.50000\n",
      "4    0.72222\n",
      "Name: label, dtype: float32\n",
      "0                       Effective but too-tepid biopic\n",
      "1    If you sometimes like to go to the movies to h...\n",
      "2    Emerges as something rare , an issue movie tha...\n",
      "3    The film provides some great insight into the ...\n",
      "4    Offers that rare combination of entertainment ...\n",
      "Name: sentence, dtype: object\n",
      "0    0.51389\n",
      "1    0.73611\n",
      "2    0.86111\n",
      "3    0.59722\n",
      "4    0.83333\n",
      "Name: label, dtype: float32\n",
      "0    It 's a lovely film with lovely performances b...\n",
      "1    No one goes unindicted here , which is probabl...\n",
      "2    And if you 're not nearly moved to tears by a ...\n",
      "3                     A warm , funny , engaging film .\n",
      "4    Uses sharp humor and insight into human nature...\n",
      "Name: sentence, dtype: object\n",
      "0    0.79167\n",
      "1    0.51389\n",
      "2    0.76389\n",
      "3    0.88889\n",
      "4    0.80556\n",
      "Name: label, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "X_train_def = train_def[\"sentence\"]\n",
    "y_train_def = train_def[\"label\"]\n",
    "X_test_def = test_def[\"sentence\"]\n",
    "y_test_def = test_def[\"label\"]\n",
    "X_val_def = val_def[\"sentence\"]\n",
    "y_val_def = val_def[\"label\"]\n",
    "print(X_train_def.head())\n",
    "print(y_train_def.head())\n",
    "print(X_test_def.head())\n",
    "print(y_test_def.head())\n",
    "print(X_val_def.head())\n",
    "print(y_val_def.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the scores to sentiment classes\n",
    "def map_sentiment(score):\n",
    "    if score <= 0.2:\n",
    "        return 0\n",
    "    elif score <= 0.4:\n",
    "        return 1\n",
    "    elif score <= 0.6:\n",
    "        return 2\n",
    "    elif score <= 0.8:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 1 0]\n",
      "[2 3 4 1 0]\n",
      "[3 2 4 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Map the scores to sentiment classes\n",
    "y_train = y_train_def.apply(map_sentiment)\n",
    "y_test = y_test_def.apply(map_sentiment)\n",
    "y_val = y_val_def.apply(map_sentiment)\n",
    "print(y_train.unique())\n",
    "print(y_test.unique())\n",
    "print(y_val.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79268\n",
      "8544\n",
      "(2210, 79268)\n",
      "(1101, 79268)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_def).astype(int)\n",
    "X_test = vectorizer.transform(X_test_def).astype(int)\n",
    "X_val = vectorizer.transform(X_val_def).astype(int)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing_factor=1):\n",
    "        self.classes = None\n",
    "        self.class_priors = None\n",
    "        self.feature_log_prob_ = None  # Changed name to match scikit-learn's convention\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.vocab = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = self.calculate_class_priors(y)\n",
    "        self.feature_log_prob_ = self.calculate_feature_log_prob(X, y)  # Changed method name\n",
    "\n",
    "    def calculate_class_priors(self, y):\n",
    "        class_counts = np.bincount(y)\n",
    "        total_samples = len(y)\n",
    "        class_priors = class_counts / total_samples\n",
    "        return class_priors\n",
    "\n",
    "    def calculate_feature_log_prob(self, X, y):  # Changed method name\n",
    "        num_features = X.shape[1]\n",
    "        feature_log_prob_ = []  \n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            log_probabilities = []\n",
    "\n",
    "            for feature in range(num_features):\n",
    "                feature_values = X_c[:, feature]\n",
    "                if feature_values.getnnz() != 0:\n",
    "                    self.vocab = vectorizer.get_feature_names_out()\n",
    "                    print(feature_values)\n",
    "                feature_counts = feature_values.getnnz(axis=0)\n",
    "                feature_probabilities = (feature_counts + self.smoothing_factor) / (X_c.shape[0] + self.smoothing_factor * num_features)\n",
    "                feature_log_probabilities = np.log(feature_probabilities)\n",
    "                log_probabilities.append(feature_log_probabilities)\n",
    "\n",
    "            feature_log_prob_.append(log_probabilities)\n",
    "\n",
    "        return np.array(feature_log_prob_).transpose()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for j in range(len(self.classes)):\n",
    "            class_prior = self.class_priors[j]\n",
    "            class_scores = []\n",
    "\n",
    "            for i in range(X.shape[0]):  # Iterate over the number of rows\n",
    "                sample = X[i]\n",
    "                log_likelihood = 0\n",
    "\n",
    "                if sample in self.vocab:\n",
    "                    for feature in range(len(sample)):\n",
    "                        if sample[feature] in self.vocab:\n",
    "                            log_likelihood += self.feature_log_prob_[j][feature]\n",
    "\n",
    "                    class_score = np.log(class_prior) + log_likelihood\n",
    "                    class_scores.append(class_score)\n",
    "                    # Sample exists in the vocabulary\n",
    "                    print(\"Sample exists in the vocabulary\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            predicted_class = self.classes[np.argmax(class_scores)]\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def log_prior(self):\n",
    "        return np.log(self.class_priors)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        return self.feature_log_prob_  # No need to modify this, as it already returns log probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (13, 0)\t1\n",
      "  (247, 0)\t1\n",
      "  (310, 0)\t1\n",
      "  (498, 0)\t1\n",
      "  (841, 0)\t1\n",
      "  (662, 0)\t1\n",
      "  (181, 0)\t1\n",
      "  (701, 0)\t1\n",
      "  (848, 0)\t1\n",
      "  (360, 0)\t1\n",
      "  (292, 0)\t1\n",
      "  (74, 0)\t1\n",
      "  (0, 0)\t1\n",
      "  (183, 0)\t1\n",
      "  (665, 0)\t1\n",
      "  (263, 0)\t1\n",
      "  (739, 0)\t1\n",
      "  (898, 0)\t1\n",
      "  (804, 0)\t1\n",
      "  (432, 0)\t1\n",
      "  (783, 0)\t1\n",
      "  (102, 0)\t1\n",
      "  (83, 0)\t1\n",
      "  (560, 0)\t1\n",
      "  (360, 0)\t1\n",
      "  (770, 0)\t1\n",
      "  (247, 0)\t1\n",
      "  (169, 0)\t1\n",
      "  (431, 0)\t1\n",
      "  (247, 0)\t1\n",
      "  (169, 0)\t1\n",
      "  (67, 0)\t1\n",
      "  (918, 0)\t1\n",
      "  (432, 0)\t1\n",
      "  (70, 0)\t1\n",
      "  (1038, 0)\t1\n",
      "  (651, 0)\t1\n",
      "  (43, 0)\t1\n",
      "  (675, 0)\t1\n",
      "  (1000, 0)\t1\n",
      "  (870, 0)\t1\n",
      "  (167, 0)\t1\n",
      "  (367, 0)\t1\n",
      "  (84, 0)\t1\n",
      "  (235, 0)\t1\n",
      "  (84, 0)\t1\n",
      "  (422, 0)\t1\n",
      "  (10, 0)\t1\n",
      "  (560, 0)\t1\n",
      "  (879, 0)\t1\n",
      "  (211, 0)\t1\n",
      "  (956, 0)\t1\n",
      "  (970, 0)\t1\n",
      "  (73, 0)\t1\n",
      "  (227, 0)\t1\n",
      "  (46, 0)\t1\n",
      "  (460, 0)\t1\n",
      "  (194, 0)\t1\n",
      "  (818, 0)\t1\n",
      "  (633, 0)\t1\n",
      "  (760, 0)\t1\n",
      "  (798, 0)\t1\n",
      "  (798, 0)\t1\n",
      "  (507, 0)\t1\n",
      "  (868, 0)\t1\n",
      "  (507, 0)\t1\n",
      "  (202, 0)\t1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m naive_bayes \u001b[38;5;241m=\u001b[39m NaiveBayes()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnaive_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m nb1 \u001b[38;5;241m=\u001b[39m naive_bayes\n",
      "Cell \u001b[1;32mIn[59], line 14\u001b[0m, in \u001b[0;36mNaiveBayes.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_priors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_class_priors(y)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_log_prob_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_feature_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 33\u001b[0m, in \u001b[0;36mNaiveBayes.calculate_feature_log_prob\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     31\u001b[0m feature_values \u001b[38;5;241m=\u001b[39m X_c[:, feature]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_values\u001b[38;5;241m.\u001b[39mgetnnz() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feature_values)\n\u001b[0;32m     35\u001b[0m feature_counts \u001b[38;5;241m=\u001b[39m feature_values\u001b[38;5;241m.\u001b[39mgetnnz(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1487\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m-> 1487\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m-> 1487\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1489\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "nb1 = naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2210,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nb1_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnb1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 62\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     59\u001b[0m         class_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(class_prior) \u001b[38;5;241m+\u001b[39m log_likelihood\n\u001b[0;32m     60\u001b[0m         class_scores\u001b[38;5;241m.\u001b[39mappend(class_score)\n\u001b[1;32m---> 62\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_scores\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     63\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(predicted_class)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2210,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "nb1_predictions = nb1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "nb2 = naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.0572184  -1.34862339 -1.66033704 -1.3028004  -1.89213865]\n",
      "[-2.0572184  -1.34862339 -1.66033704 -1.3028004  -1.89213865]\n",
      "[[-11.47047715 -11.47047715 -11.47047715 ... -11.47047715 -11.47047715\n",
      "  -10.77732997]\n",
      " [-11.63595693 -10.94280975 -10.94280975 ... -11.63595693 -11.63595693\n",
      "  -11.63595693]\n",
      " [-10.8486961  -11.54184329 -11.54184329 ... -10.8486961  -11.54184329\n",
      "  -11.54184329]\n",
      " [-11.65872134 -11.65872134 -11.65872134 ... -11.65872134 -11.65872134\n",
      "  -11.65872134]\n",
      " [-11.49646066 -11.49646066 -11.49646066 ... -11.49646066 -10.80331348\n",
      "  -11.49646066]]\n",
      "[[-11.29427182 -11.29427182 -11.29427182 ... -11.29427182 -11.29427182\n",
      "  -10.60112464]\n",
      " [-11.30818651 -10.61503932 -10.61503932 ... -11.30818651 -11.30818651\n",
      "  -11.30818651]\n",
      " [-10.60772303 -11.30087021 -11.30087021 ... -10.60772303 -11.30087021\n",
      "  -11.30087021]\n",
      " [-11.30946198 -11.30946198 -11.30946198 ... -11.30946198 -11.30946198\n",
      "  -11.30946198]\n",
      " [-11.29670787 -11.29670787 -11.29670787 ... -11.29670787 -10.60356069\n",
      "  -11.29670787]]\n"
     ]
    }
   ],
   "source": [
    "print(nb2.class_log_prior_)\n",
    "print(nb1.log_prior())\n",
    "print(nb2.feature_log_prob_)\n",
    "print(nb1.log_likelihood()[0].transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same logs so both models work the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, num_features):\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.initialize_parameters(num_features)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = X.dot(self.weights.T) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            \n",
    "            dw = (1 / num_samples) * X.T.dot((y_pred - y))\n",
    "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = X.dot(self.weights.T) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        y_pred_class = np.where(y_pred > 0.5, 1, 0)\n",
    "        return y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegressionClassifier()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "lr1 = lr_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Logistic Regression from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr2 = lr_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(max_iter=100, tol=1e-3, shuffle=True, random_state=42))\n",
    "lr3 = lr_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix &Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix1(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix2(y_true, y_pred):\n",
    "    labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    num_labels = len(labels)\n",
    "    cm = np.zeros((num_labels, num_labels), dtype=int)\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_label = np.where(labels == y_true[i])[0][0]\n",
    "        pred_label = np.where(labels == y_pred[i])[0][0]\n",
    "        cm[true_label][pred_label] += 1\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(confusion_matrix):\n",
    "    # Get the unique labels\n",
    "    labels = confusion_matrix.index.tolist()\n",
    "\n",
    "    # Initialize lists to store precision, recall, and F1 score per class\n",
    "    precision_per_class = []\n",
    "    recall_per_class = []\n",
    "    f1_score_per_class = []\n",
    "\n",
    "    # Compute precision, recall, and F1 score per class\n",
    "    for label in labels:\n",
    "        true_positives = confusion_matrix.loc[label, label]\n",
    "        false_positives = confusion_matrix.loc[:, label].sum() - true_positives\n",
    "        false_negatives = confusion_matrix.loc[label, :].sum() - true_positives\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        precision_per_class.append(precision)\n",
    "        recall_per_class.append(recall)\n",
    "        f1_score_per_class.append(f1)\n",
    "\n",
    "    # Compute macro-averaged precision, recall, and F1 score\n",
    "    macro_precision = sum(precision_per_class) / len(precision_per_class)\n",
    "    macro_recall = sum(recall_per_class) / len(recall_per_class)\n",
    "    macro_f1_score = sum(f1_score_per_class) / len(f1_score_per_class)\n",
    "\n",
    "    return {\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_score_per_class': f1_score_per_class,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1_score': macro_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[301], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m nb1 \u001b[38;5;66;03m# Change to nb2, lr1, or lr2 to generate confusion matrix for other models\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[297], line 52\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     50\u001b[0m class_prior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_priors[i]\n\u001b[0;32m     51\u001b[0m feature_log_prob_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_log_prob_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT[i]\n\u001b[1;32m---> 52\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(feature_log_prob_[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m), sample])  \u001b[38;5;66;03m# Removed np.log\u001b[39;00m\n\u001b[0;32m     53\u001b[0m class_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(class_prior) \u001b[38;5;241m+\u001b[39m log_likelihood\n\u001b[0;32m     54\u001b[0m class_scores\u001b[38;5;241m.\u001b[39mappend(class_score)\n",
      "File \u001b[1;32mc:\\Users\\eyada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:395\u001b[0m, in \u001b[0;36m_spbase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "model = nb1 # Change to nb2, lr1, or lr2 to generate confusion matrix for other models\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = generate_confusion_matrix1(y_test, y_pred)\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm1, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = generate_confusion_matrix2(y_test, y_pred)\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm2, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = compute_metrics(cm2)\n",
    "precision_scratch, recall_scratch, f1_scratch = Metrics['precision_per_class'], Metrics['recall_per_class'], Metrics['f1_score_per_class']\n",
    "# Calculate precision, recall, and F1 score using sklearn functions\n",
    "precision_sklearn = precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall_sklearn = recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1_sklearn = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "# Compare the metrics\n",
    "print(\"Precision (from scratch):\", precision_scratch)\n",
    "print(\"Precision (sklearn):\", precision_sklearn)\n",
    "print(\"Recall (from scratch):\", recall_scratch)\n",
    "print(\"Recall (sklearn):\", recall_sklearn)\n",
    "print(\"F1 Score (from scratch):\", f1_scratch)\n",
    "print(\"F1 Score (sklearn):\", f1_sklearn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
